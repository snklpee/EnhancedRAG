{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7062c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Cargo is Rust’s build system and package manager. Most Rustaceans use this tool to manage their Rust projects because Cargo handles a lot of tasks for you, such as building your code, downloading the libraries your code depends on, and building those libraries. (We call the libraries that your code needs dependencies.)\\n\\nThe simplest Rust programs, like the one we’ve written so far, don’t have any dependencies. If we had built the “Hello, world!” project with Cargo, it would only use the part of Cargo that handles building your code. As you write more complex Rust programs, you’ll add dependencies, and if you start a project using Cargo, adding dependencies will be much easier to do.\\n\\nBecause the vast majority of Rust projects use Cargo, the rest of this book assumes that you’re using Cargo too. Cargo comes installed with Rust if you used the official installers discussed in the “Installation” section. If you installed Rust through some other means, check whether Cargo is installed by entering the following in your terminal:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def835c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0ba805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42830527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter.from_huggingface_tokenizer(tokenizer=hf_tokenizer,chunk_overlap=50, tokens_per_chunk=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862900f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "tokens = splitter.count_tokens(text=\"whats up ?\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d3df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "chunks = splitter.split_text(text)\n",
    "print(len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_chunker (txt, hf_tokenizer, chunk_size: int, chunk_overlap: int):\n",
    "    \"\"\"makes chunks from text\"\"\"\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_tokenizer),\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(txt)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def embed_query(text: str):\n",
    "    \n",
    "    embedded_query = embedding_model.embed_query(text)\n",
    "    dimension = len(embedded_query)\n",
    "    \n",
    "    return embedded_query, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6bcc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query, dimension = embed_query(\"what is it really like ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb57293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01067e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
